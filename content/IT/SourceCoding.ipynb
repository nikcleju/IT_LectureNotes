{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{math}\n",
    "\\newcommand{\\snII}[5]{#1: \\left( \\begin{matrix} {#2} & {#4} \\\\ #3 & #5 \\end{matrix} \\right)}\n",
    "\\newcommand{\\snIII}[7]{#1: \\left( \\begin{matrix} {#2} & {#4} & {#6} \\\\ #3 & #5 & #7 \\end{matrix} \\right)}\n",
    "\\newcommand{\\snIV}[9]{#1:  \\left( \\begin{matrix} {#2} & {#4} & {#6} & {#8} \\\\ #3 & #5 & #7 & #9 \\end{matrix} \\right)}\n",
    "\n",
    "\\newcommand{\\sII}[3] {#1: \\left( \\begin{matrix} s_1 & s_2 \\\\ #2 & #3 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sIII}[4] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 \\\\ #2 & #3 & #4 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sIV}[5] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 \\\\ #2 & #3 & #4  & #5 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sVI}[7] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 & s_5 & s_6 \\\\ #2 & #3 & #4 & #5 & #6 & #7\\end{matrix} \\right)}\n",
    "\\newcommand{\\sVIII}[9] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 & s_5 & s_6 & s_7 & s_8\\\\ #2 & #3 & #4 & #5 & #6 & #7 & #8 & #9 \\end{matrix} \\right)}\n",
    "\\newcommand{\\fIoII}{\\frac{1}{2}}\n",
    "\\newcommand{\\fIoIII}{\\frac{1}{3}}\n",
    "\\newcommand{\\fIoIV}{\\frac{1}{4}}\n",
    "\\newcommand{\\fIoV}{\\frac{1}{5}}\n",
    "\\newcommand{\\fIoVI}{\\frac{1}{6}}\n",
    "\\newcommand{\\fIoVII}{\\frac{1}{7}}\n",
    "\\newcommand{\\fIoVIII}{\\frac{1}{8}}\n",
    "```\n",
    "\n",
    "# Source Coding\n",
    "\n",
    "In this chapter we take a look at the basic principles\n",
    "and algorithms for data compression.\n",
    "\n",
    "## The role of coding\n",
    "\n",
    "In the general block diagram of a communication system, \n",
    "the coding block is situated \n",
    "between the information source and the communication channel.\n",
    "\n",
    "![Communication system](img/CommBlockDiagram.png){width=35%}\n",
    "\n",
    "It's role is to prepare the raw information in order to be transmitted\n",
    "over the channel. It has two main jobs:\n",
    "\n",
    "1. **Source coding**:\n",
    "   - Convert source messages to channel symbols, i.e. the actual \n",
    "     symbols which the channel knows to transmit.\n",
    "\n",
    "     For example, express the messages in binary form (zeros and ones), for sending over a binary channel.\n",
    "\n",
    "   - Minimize the number of symbols needed to be transmitted (i.e., data compression). \n",
    "     We don't want to transmit more symbols than necessary to recover the messages at the receiving end.\n",
    " \n",
    "   - Adapt probabilities of symbols in order to maximize to maximize mutual information. \n",
    "   We will discuss this more in Chapter IV.\n",
    "\n",
    "2. **Error control coding**\n",
    "    - Protect the information against channel errors\n",
    "    - Also known as \"*channel coding*\"\n",
    "\n",
    "Basically, source coding refers to all the procedures required to express\n",
    "the source messages as channel symbols in the most efficient way possible,\n",
    "while error control coding refers to all the algorithms used to\n",
    "protect the data against errors.\n",
    "\n",
    "The coding block has a corresponding decoding block on the receiving end.\n",
    "Its job is to \"*undo*\" all the coding operations:\n",
    "\n",
    "- detect and fix the errors in the received data, based on the algorithms introduced by the coding block\n",
    "- convert the channel symbols back into the message representations that the receiver expects\n",
    "\n",
    "Is it possible to do these two jobs separately, one after another, \n",
    "in two consecutive operations? Yes, as the **source-channel separation theorem** establishes.\n",
    "\n",
    "We give below only an informal statement of the theorem:\n",
    "\n",
    "```{prf:theorem} Source-channel separation theorem\n",
    "It is possible to obtain the best reliable communication by performing the\n",
    "two tasks **separately**:\n",
    "\n",
    "1. Source coding: to minimize number of symbols needed\n",
    "2. Error control coding (channel coding): to provide protection against errors happening on the channel\n",
    "```\n",
    "\n",
    "In this chapter, we consider only the source coding algorithms, without any error control.\n",
    "Basically, we assume that data transmission is done over an ideal channel with no noise, \n",
    "and therefore the transmitted symbols are perfectly recovered at the receiver.\n",
    "\n",
    "In this context, our main concern is how to minimize the number of symbols needed to represent the messages, while making sure that the receiver can decode the messages correctly. \n",
    "The advantages of data compression are self-evident:\n",
    "\n",
    "- Efficiency\n",
    "- Short communication times\n",
    "- Can decode easily\n",
    "\n",
    "\n",
    "## Definitions\n",
    "\n",
    "Let's define what coding means from a mathematical perspective.\n",
    "\n",
    "Consider an input information source with the set of messages:\n",
    "\n",
    "$$S = \\left\\lbrace s_1, s_2, ... s_N \\right\\rbrace$$\n",
    "\n",
    "and suppose we would like to express the messages as a sequence of the following **code symbols**:\n",
    "\n",
    "$$X = \\left\\lbrace x_1, x_2, ... x_M \\right\\rbrace$$\n",
    "\n",
    "The set $X$ is known as the **alphabet of the code**.\n",
    "\n",
    "For example, for a binary code we have $X = \\lbrace 0, 1\\rbrace$ \n",
    "and a possible sequence of symbols is $c = 00101101$\n",
    "\n",
    "```{prf:definition} Code definition\n",
    "A **code** is a mapping from the set $S$ of $N$ messages\n",
    "to a set of $N$ sequences of symbols, known as **codewords**:\n",
    "\n",
    "$$C = \\left\\lbrace c_1, c_2, ... c_N \\right\\rbrace$$\n",
    "\n",
    "An example code mapping is given below: \n",
    "\n",
    " Message   |               | Codeword\n",
    "---------: | :----------:  | :----------\n",
    "$s_1$      | $\\rightarrow$ | $c_1 = x_1x_2x_1...$ \n",
    "$s_2$      | $\\rightarrow$ | $c_2 = x_1x_2x_2...$ \n",
    "$\\dots$    | $\\rightarrow$ | $\\dots$ \n",
    "$s_N$      | $\\rightarrow$ | $c_N = x_2x_2x_2...$ \n",
    "\n",
    "```\n",
    "\n",
    "The codewords are the sequences of symbols used by the code.\n",
    "\n",
    "The **codeword length**, which we denote as $l_i$, \n",
    "is the the number of symbols in a given codeword $c_i$.\n",
    "\n",
    "\n",
    "**Encoding** a given message or sequences of messages means \n",
    "replacing each message with its codeword.\n",
    "\n",
    "**Decoding** means deducing back the original sequence of messages,\n",
    "given a sequence of symbols.\n",
    "\n",
    "As an example, the ASCII code is a widely-used code for encoding characters,\n",
    "consisting of 256 codewords (stored on 1 byte):\n",
    "\n",
    "<!-- ![ASCII code (partial)](img/ASCIIcode.png){width=50%} -->\n",
    "![ASCII code (partial)](img/ASCIIcode.png)\n",
    "\n",
    "Nowadays, ASCII is usually replaced by Unicode (UTF-8), which is a more general code using 65536 codewords (stored on 2 bytes), allowing for many more letters used in languages around the globe.\n",
    "\n",
    "### The graph of a code\n",
    "\n",
    "The codewords of a code can represented as a binary tree.\n",
    "We call this representation **the graph of the code**. \n",
    "\n",
    "Example at blackboard\n",
    "\n",
    "### Average code length\n",
    "\n",
    "Consider a code for the messages of a discrete memoryless source.\n",
    "There are many ways to define the codewords and their mapping to messages.\n",
    "\n",
    "How to measure representation efficiency of a code? We need this, for example, \n",
    "in order to choose the most efficient code out of all possible codes we might define.\n",
    "\n",
    "The most basic quantity indicating efficiency of a code is the average code length.\n",
    "\n",
    "```{prf:definition} Average code length\n",
    "Given a code, the **average code length** is the average of the codeword lengths:\n",
    "\n",
    "$$\\overline{l} = \\sum_i p(s_i) l_i$$\n",
    "```\n",
    "\n",
    "Here, for every codeword $c_i$ we consider its probability\n",
    "to be the probability of the corresponding message $p(s_i)$, and its length $l_i$.\n",
    "\n",
    "A code with smaller average length is better, because it represents sequences of messages\n",
    "with less symbols, on average. However, we expect a certain \n",
    "lower limit to the average length (for example, it cannot be 0, for self-evident reasons).\n",
    "This raises the following interesting question:\n",
    "\n",
    "> Given a source $S$, how small can the average length be?\n",
    "\n",
    "This is a fundamental question, to which we will provide an answer later in this chapter.\n",
    "\n",
    "### Instantaneous codes\n",
    "\n",
    "We introduce another set of useful definitions regarding the codeword structure.\n",
    "\n",
    "A code is:\n",
    "\n",
    "- **non-singular**: all codewords are different\n",
    "- **uniquely decodable**: for any received sequence of symbols, there is only one corresponding sequence of messages\n",
    "  - i.e. no sequence of messages produces the same sequence of symbols\n",
    "  - i.e. there is never a confusion at decoding\n",
    "- **instantaneous** (also known as **prefix-free**): no codeword is prefix to another code\n",
    "  - A **prefix** = a codeword which is the beginning of another codeword\n",
    "\n",
    "Examples: at the blackboard\n",
    "\n",
    "The follwing relations exist between these types of codes.\n",
    "\n",
    "```{prf:theorem} Instantaneous codes are uniquely decodable\n",
    "An instantaneous code is uniquely decodable\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "\n",
    "There is exactly one codeword matching the beginning of the sequence:\n",
    "\n",
    "- Suppose the true initial codeword is **c**\n",
    "- There can't be a shorter codeword **c'**, since it would be prefix to **c**\n",
    "- There can't be a longer codeword **c''**, since **c** would be prefix to it\n",
    "\n",
    "Once we find the first codeword, write down the corresponding message and remove the codeword from the sequence.\n",
    "\n",
    "The remaining part is another sequence and, by the same argument, there is exactly one codeword matching the new beginning, and so on.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "The converse is not necessary true; there exist uniquely decodable codes which\n",
    "are not instantaneous.\n",
    "```\n",
    "\n",
    "```{prf:theorem} Uniquely decodable codes are non-singular\n",
    "An uniquely decodable code is non-singular\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "The proof is by contradiction:\n",
    "\n",
    "- If the code is singular, some codewords are not unique (different messages have the same codeword)\n",
    "- This means that at decoding we can't decide which of those messages is there. \n",
    "  This means that the code is not uniquely decodable\n",
    "- Therefore, if the code is uniquely-decodable, it must also be non-singular ($A \\rightarrow B \\Leftrightarrow \\overline{B} \\rightarrow \\overline{A}$)\n",
    "```\n",
    "\n",
    "We can summarize the relation between these three code types as follows:\n",
    "\n",
    "Instantaneous $\\subset$ uniquely decodable $\\subset$ non-singular\n",
    "\n",
    "\n",
    "### Graph-based decoding of instantaneous codes\n",
    "\n",
    "Using the graph of the code, we can use a very simple procedure for decoding any instantaneous code:\n",
    "\n",
    "```{prf:algorithm} Graph-based decoding of instantaneous codes\n",
    ":label: graph-based-decoding\n",
    "\n",
    "**Inputs** An input symbol sequence, the graph of an instantaneous code\n",
    "\n",
    "**Output** The decoded message sequence\n",
    "\n",
    "1. Start at the root of the tree graph\n",
    "2. Follow the edges in the tree according to the next symbols in the sequence\n",
    "3. When a message is reached in the tree, write it down and go back to the root\n",
    "4. Continue until the end of the symbol sequence\n",
    "```\n",
    "\n",
    "TBD: Illustrate at whiteboard\n",
    "\n",
    "This procedure shows the advantage of instantaneous codes over other codes which\n",
    "might be uniquely decodable, but are not instantaneous: \n",
    "instantaneous codes allow for **simple decoding**. There is never any doubt about\n",
    "the next message in the sequence.\n",
    "\n",
    "This decoding scheme is also showing why these codes are named \"*instantaneous*\":\n",
    "a codeword can be decoded as soon as it is fully received, immediately, without any delay.\n",
    "\n",
    "As a counter example, consider the following uniquely decodable, but non-instantaneous code: $\\left\\lbrace 0, 01, 011, 1110 \\right\\rbrace$. When you read the first $0$, you cannot decode it yet, because you need to wait the next bits to understand how to segment the sequence. This implies that the decoding has some delay.\n",
    "\n",
    "### The Kraft inequality theorem\n",
    "\n",
    "When can an instantaneous code exist? Given a DMS $S$, are we sure we can find an \n",
    "instantaneous code for it, and if yes, under which conditions?\n",
    "\n",
    "The answers to these questions is provided by the Kraft inequality.\n",
    "\n",
    "```{prf:theorem} Kraft inequality theorem\n",
    "Given a code alphabet of $D$ symbols, there exists an instantaneous code with codeword lengths ${l_1, l_2, \\ldots l_n}$ \n",
    "if and only if the lengths satisfy the following inequality:\n",
    "\n",
    "$$\\sum_i D^{-l_i} \\leq 1$$\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "At blackboard\n",
    "```\n",
    "\n",
    "Comments on the Kraft inequality:\n",
    "\n",
    "- If lengths do not satisfy the relation, no instantaneous code exists with these lengths\n",
    "- If the lengths of a code satisfy the relation, that code can be instantaneous or not (there   exists an instantaneous code, but not necessarily that one).\n",
    "  Keep in mind that the Kraft inequality only looks at the lengths of the codewords, not\n",
    "  at their actual symbols, so it can only say something about the lengths, not about the actual codewords\n",
    "- The Kraft inequality implies that the codewords lengths cannot be all very small, because if\n",
    "  all $l_i$ values are too small the sum exceeds 1. Thus, implicitly, it sets a lower limit to the permissible lengths.\n",
    "\n",
    "- From the proof, it follows that we have equality in the relation\n",
    "  \n",
    "  $$ \\sum_i D^{-l_i} = 1$$\n",
    "  \n",
    "  only if the lowest level of the tree is fully covered.\n",
    "  Thus, for an instantaneous code which satisfies Kraft with equality, \n",
    "  all the graph branches terminate with codewords and there are no unused branches.\n",
    "  \n",
    "  This makes sense intuitively, since is most economical way: codewords are as short as they can be. Any unused branch means that we can actually make the code shorter by moving some message up the tree.\n",
    "\n",
    "We have seen that instantaneous codes must obey the Kraft inequality\n",
    "But how about uniquely decodable codes? \n",
    "The answer is given by the next theorem.\n",
    "\n",
    "```{prf:theorem} McMillan theorem\n",
    "Any uniquely decodable code **also** satisfies the Kraft inequality:\n",
    "\n",
    "$$ \\sum_i D^{-l_i} \\leq 1.$$\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "No proof given\n",
    "```\n",
    "\n",
    "Consequences of the McMillan theorem:\n",
    "\n",
    "- For every uniquely decodable code, there exists in instantaneous code\n",
    "with the same lengths. This is because the lengths $l_i$ must satisfy the same\n",
    "relation both for unique-decodable and for instantanous codes.\n",
    "\n",
    "- Thus, even though the class of uniquely decodable codes is larger than that of\n",
    "instantaneous codes (because any instantaneous code is uniquely decodable, but not\n",
    "any uniquely decodable code is instantaneous), using uniquely-decodable codes\n",
    "brings **no additional benefit** in average codeword length\n",
    "\n",
    "- Instead of an uniquely decodable code, we can always use an instantaneous code, \n",
    "which has the same lengths, but is much easier to decode.\n",
    "\n",
    "### Finding an instantaneous code for given lengths\n",
    "\n",
    "How to find codewords with code lengths $\\{l_i\\}$?\n",
    "\n",
    "In general, we may use the following procedure:\n",
    "\n",
    "```{prf:algorithm}\n",
    "1. Check that lengths satisfy Kraft relation\n",
    "2. Draw graph with the specified lengths\n",
    "3. Assign codewords to the graph terminations\n",
    "```\n",
    "\n",
    "Note that this procedure only gives us the codewords, not the mapping\n",
    "to a particular set of messages.\n",
    "\n",
    "In practice, there might be more elaborate ways to find the codewords and map them\n",
    "to the messages of the source, with additional benefits.\n",
    "\n",
    "### Optimal codes\n",
    "\n",
    "We will discuss now one of the most important aspect of this chapter,\n",
    "\n",
    "Given a DMS $S$, suppose we want to find an instantenous code for it, \n",
    "but in such a way as to **minimize the average length** of the code:\n",
    "\n",
    "$$\\overline{l} = \\sum_i p(s_i) l_i$$\n",
    "\n",
    "How can we find such an optimal code?\n",
    "\n",
    "Given that it should be instantaneous, the codeword lengths must obey the Kraft inequality.\n",
    "\n",
    "In mathenatical terms, we formulate the problem as a **constrained optimization problem**:\n",
    "\n",
    "$$\\begin{aligned} \\textbf{minimize } &\\sum_i p(s_i) l_i \\\\\n",
    "\\textrm{subject to } &\\sum_i D^{-l_i} \\leq 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that we want to find the unknowns $l_i$ \n",
    "in order to minimize a certain quantity ($\\sum_i p(s_i) l_i$),\n",
    "but the unknowns must satisfy a certain constraint ($\\sum_i D^{-l_i} \\leq 1$).\n",
    "\n",
    "#### The method of Lagrange multipliers\n",
    "\n",
    "To solve this problem, we use a standard mathematical tool known as the **method of Lagrange multipliers**.\n",
    "\n",
    "```{prf:definition} Lagrange multipliers for constrained optimization problems\n",
    "\n",
    "To solve the following constrained optimization problem:\n",
    "\n",
    "$$\\begin{aligned} \\textbf{minimize } & f(x) \\\\\n",
    "\\textrm{subject to } & g(x) = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "one must build a new function $L(x, \\lambda)$ (known as the **Lagrangean function**):\n",
    "\n",
    "$$L(x, \\lambda) = f(x) - \\lambda g(x)$$\n",
    "\n",
    "The solution $x$ of the original problem is among the solutions of the system:\n",
    "\n",
    "$$\n",
    "\\begin{cases} \\frac{\\partial L(x, \\lambda)}{\\partial x} &= 0 \\\\\n",
    "\\frac{\\partial L(x, \\lambda)}{\\partial \\lambda} &= 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If there are multiple variables $x_i$ in the problem, we take the partial derivative\n",
    "$\\frac{\\partial L(x, \\lambda)}{\\partial x_i}$ for each one of them, resulting in a larger system.\n",
    "```\n",
    "\n",
    "Let's use this mathematical formulation to our problem. \n",
    "In our case, the functions and the variables involved are the following:\n",
    "\n",
    "- The unknowns $x_i$ are the lengths $l_i$\n",
    "- The function to minimize is $f(x) = f(l_i) = \\overline{l} = \\sum_i p(s_i) l_i$\n",
    "- The constraint is $g(x) = g(l_i) = \\sum_i D^{-l_i} - 1$\n",
    "\n",
    "```{note}\n",
    "The method of Lagrange multipliers, as presented below, specifies an equality constraint:\n",
    "\n",
    "$$\n",
    "\\textrm{subject to } g(x) = 0\n",
    "$$\n",
    "\n",
    "But in our optimization problem, we have an inequality constraint ($\\leq$ instead of $=$):\n",
    "\n",
    "$$\n",
    "\\textrm{subject to } \\sum_i D^{-l_i} \\leq 1\n",
    "$$\n",
    "\n",
    "However, as we discussed earlier, having the Kraft inequality smaller than $1$ means that we have\n",
    "some coding inefficiency, since some branches of the graph code remaining unconnected.\n",
    "When we seek to minimize the average length, this cannot happen. For this reason,\n",
    "we can safely use equality here in the Kraft constraint:\n",
    "\n",
    "$$\n",
    "\\textrm{subject to } \\sum_i D^{-l_i} = 1\n",
    "$$\n",
    "```\n",
    "\n",
    "```{note}\n",
    "A keen reader might observe a requirement which our optimization problem does not \n",
    "take into account: the variables $l_i$ cannot just take any value, they must be positive integers,\n",
    "since they are the length of some codewords. \n",
    "\n",
    "We will discuss this problem in the next paragraphs.\n",
    "```\n",
    "\n",
    "**TBD: Solve at blackboard**\n",
    "\n",
    "The resulting optimal codeword lengths are:\n",
    "\n",
    "$$\n",
    "l_i = -\\log(p(s_i))\n",
    "$$\n",
    "\n",
    "```{prf:theorem}\n",
    "The optimal codeword lenghts $l_i$ for the messages of a DMS in order to minimize the average codeword length $\\overline{l}$ are:\n",
    "\n",
    "$$l_i = -\\log(p(s_i))$$\n",
    "\n",
    "```\n",
    "\n",
    "Let us discuss some intuitions and consequences related to this result:\n",
    "\n",
    "- Using $l_i = -\\log(p(s_i))$ satisfies Kraft with equality, so the lengths cannot be any shorter than that, in general:\n",
    "  \n",
    "  $$\n",
    "  \\textrm{subject to } \\sum_i D^{-(-\\log(p(s_i))))} = 1\n",
    "  $$\n",
    "\n",
    "\n",
    "- A message with higher probability must have a shorter codeword, while a message with lower probability has a longer codeword.\n",
    "  This makes sense because messages we can afford to use a longer codeword for messages which appear rarely in a sequence.\n",
    "\n",
    "```{margin}\n",
    "Think of examples in languages.\n",
    "\n",
    "- words which are very common are usually short: \"yes\", \"no\", \"and\", \"the\" \"with\"\n",
    "- we often use abbreviations to replace longer words which we must use often\n",
    "```\n",
    "\n",
    "\n",
    "#### Minimal average length\n",
    "\n",
    "The most important consequence of this result gives us a functional\n",
    "definition of the entropy, i.e. it tells us what the entropy really\n",
    "says about an information source.\n",
    "\n",
    "If the optimal values are $l_i = -\\log(p(s_i))$, then the minimal average length is:\n",
    "\n",
    "$$\\min \\overline{l} = \\sum_i p(s_i) l_i = -\\sum_i p(s_i) \\log(p(s_i)) = H(S)$$\n",
    "\n",
    "```{prf:corollary}\n",
    "The entropy of a discrete memoryless information source is the **minimum average length** that can ever be achieved by a instantaneous or uniquely-decodable code used to encode the messages.\n",
    "```\n",
    "\n",
    "This result tells us what the entropy really means, besides the formulaic definition provided in chapter I. The entropy gives us the minimum number of bits required to represent the data generated by an information source in binary form. Although we rigorously proved this result only for a discrete memoryless source, it holds for sources with memory as well.\n",
    "\n",
    "```{margin}\n",
    "Question:\n",
    "\n",
    "Can you sketch the proof for this for sources with memory?\n",
    "```\n",
    "\n",
    "Immediate consequences:\n",
    "\n",
    "- Messages from a source with small entropy can be written (encoded) with few bits\n",
    "- A source with with large entropy requires more bits for encoding the messages\n",
    "\n",
    "Reformulating this in a different way, we can say that he average length of an uniquely decodable code must be at least as large as the source entropy. One can never represent messages, on average, with a code  having average length less than the entropy.\n",
    "\n",
    "$$H(S) \\leq \\overline{l}$$\n",
    "\n",
    "```{prf:example}\n",
    "You can think of the relation between information (entropy) and code length\n",
    "using the analogy of water and a bottle.\n",
    "\n",
    "We use a code in order to represent and transfer information, \n",
    "just like we use a bottle to carry and store some shapeless quantity of water. \n",
    "Information is not the codewords. \n",
    "Information is a mathematical quantity implied by the random occurrence of messages, and the codewords are just means of transportation of information.\n",
    "\n",
    "A quantity of 1 liter of water can be carried with any bottle of size $\\geq$ 1 liter, but it does not fit in any bottle smaller than this. \n",
    "Similarly, an amount of information (entropy) can be encoded with a code\n",
    "for which the average length is larger or equal, but does not \"fit\" in any code smaller than this.\n",
    "```\n",
    "\n",
    "#### Efficiency and redundancy of a code\n",
    "\n",
    "Considering a code for an information source, comparing the average code length\n",
    "with the entropy indicates the efficiency of the encoding. These quantities indicate how close is the average length to the optimal value.\n",
    "\n",
    "```{prf:definition} Efficiency and redundancy of a code\n",
    "**Efficiency** of a code is defined as:\n",
    "\n",
    "$$\\eta = \\frac{H(S)}{\\overline{l} \\cdot \\log(M)}$$\n",
    "\n",
    "where $M$ is size of code alphabet (number of symbols in the code).\n",
    "\n",
    "The **redundancy** of a code is:\n",
    "\n",
    "$$\\rho = 1- \\eta$$\n",
    "\n",
    "For binary codes $M = 2$ so $\\eta$ is simply the ratio $\\eta = \\frac{H(S)}{\\overline{l}}$.\n",
    "\n",
    "For $M > 2$ a factor of $\\log(M)$ is needed because we need to express both quantities with the same unit. The entropy $H(S)$ in bits (due to the binary logarithm), but $\\overline{l}$ is not in bits, since is the average number of symbols and there are more than 2 symbols, hence the $\\log(M)$.\n",
    "```\n",
    "\n",
    "#### Optimal and non-optimal codes\n",
    "\n",
    "A code for which $\\eta = 1$ is known as an **optimal code**.\n",
    "Such a code attains the minimum average length:\n",
    "\n",
    "$$\\overline{l} = H(S)$$\n",
    "\n",
    "However, this is only possible if the probabilities are powers of 2,\n",
    "such that $l_i = -log2(p(s_i))$ is an integer number, because the codeword\n",
    "lengths must always be natural numbers.\n",
    "\n",
    "- e.g. probabilities like $1/2$, $1/4$, $1/2^n$, known as *dyadic distribution*\n",
    "- the lengths $l_i = -\\log(p(s_i))$ are all natural numbers\n",
    "\n",
    "An optimal code can always be found for a source where all $p(s_i)$ are powers of 2, using for example the graph-based procedure explained earlier.\n",
    "\n",
    "However, in the general case  $l_i = -\\log(p(s_i))$ might not be an integer number. This happens when $p(s_i)$ is not a power of 2. What to do in this case?\n",
    "\n",
    "A simple solution to this problem is to round every logarithm to next largest natural number:\n",
    "\n",
    "$$l_i = \\lceil -\\log(p(s_i)) \\rceil$$\n",
    "\n",
    "for example:\n",
    "\n",
    "$$-\\log(p(s_i)) = 2.15 => l_i = 3$$\n",
    "\n",
    "This leads to the Shannon coding algorithm described below.\n",
    "\n",
    "### Shannon coding\n",
    "\n",
    "```{prf:algorithm} Shannon coding algorithm\n",
    "**Inputs** A DMS source with probabilities $p(s_i)$\n",
    "\n",
    "**Outputs** Codewords $l_i$\n",
    "\n",
    "1. Arrange probabilities in descending order\n",
    "2. Compute the codeword lengths $l_i = \\lceil -\\log(p(s_i)) \\rceil$\n",
    "3. For every message $s_i$ find the codeword as follows:\n",
    "\n",
    "    1. sum all the probabilities up to this message (not including current)\n",
    "    2. multiply the sum value with $2^{l_i}$  \n",
    "    3. floor the result, convert to binary, retain first $l_i$ bits\n",
    "\n",
    "```\n",
    "The code obtained with this procedure is known as a **Shannon code**.\n",
    "\n",
    "Shannon coding is one of the simplest algorithms, and some other better schemes are available. However, this simplicity allows to prove fundamental results\n",
    "in coding theory, as we shall see below.\n",
    "\n",
    "```{margin}\n",
    "As an example of Shannon coding deficiency, compute the Shannon algorithm for the following source:\n",
    "\n",
    "$$S: (0.9, 0.1)$$\n",
    "```\n",
    "\n",
    "One such fundamental result shows that the average length of a Shannon code is actually close to the entropy of the source.\n",
    "\n",
    "```{prf:theorem}\n",
    "The average length $\\overline{l}$ of a Shannon code satisfies:\n",
    "\n",
    "$$H(S) \\leq \\overline{l} < H(S) + 1$$\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "\n",
    "The proof is as follows:\n",
    "\n",
    "1. The first inequality is because $H(S)$ is minimum average length\n",
    "2. The second inequality:\n",
    "    a. Use Shannon code:\n",
    "$$l_i = \\lceil -\\log(p(s_i)) \\rceil = -\\log(p(s_i)) + \\epsilon_i$$ where $0 \\leq \\epsilon_i < 1$\n",
    "\n",
    "    a. Compute average length:\n",
    "$$\\overline{l} = \\sum_i p(s_i) l_i = H(S) + \\underbrace{\\sum_i p(s_i) \\epsilon_i}_{< 1}$$\n",
    "    a. Since $\\epsilon_i < 1$ => $\\sum_i p(s_i) \\epsilon_i < \\sum_i p(s_i)  = 1$ \\qed\n",
    "```\n",
    "\n",
    "* Average length of Shannon code is **at most 1 bit longer** than the minimum possible value\n",
    "    * That's quite efficient\n",
    "    * There exist even better codes, in general\n",
    "    \n",
    "* Q: Can we get even closer to the minimum length?\n",
    "* A: Yes, as close as we want!\n",
    "    * In theory, at least ... :)\n",
    "    * See next slide.\n",
    "\n",
    "### Shannon's first theorem\n",
    "\n",
    "Shannon's first theorem (coding theorem for noiseless channels):\n",
    "\n",
    "* It is possible to encode an infinitely long sequences of messages \n",
    "from a source S with an average length \n",
    "as close as desired to H(S), \n",
    "but never below H(S)\n",
    "\n",
    "\\ \n",
    "\\\n",
    "\n",
    "Key points:\n",
    "\n",
    "  * we can always obtain $\\overline{l} \\to H(S)$\n",
    "  * for an infinitely long sequence\n",
    "\n",
    "### Shannon's first theorem\n",
    "\n",
    "Proof:\n",
    "\n",
    "* Average length can never go below H(S) because this is minimum\n",
    "* How can it get very close to H(S) (from above)?\n",
    "    1. Use **$n$-th order extension** $S^n$ of S\n",
    "    2. Use Shannon coding for $S^n$, so it satisfies\n",
    "$$H(S^n) \\leq \\overline{l_{S^n}} < H(S^n) + 1$$\n",
    "    3. But $H(S^n) = n H(S)$, and average length **per message of $S$** is\n",
    "$$\\overline{l_{S}} = \\frac{\\overline{l_{S^n}}}{n}$$\n",
    "because messages of $S^n$ are just $n$ messages of $S$ glued together\n",
    "    4. So, dividing by $n$:\n",
    "$$\\boxed{H(S) \\leq \\overline{l_{S}} < H(S) + \\frac{1}{n}}$$\n",
    "    5. If extension order $n \\to \\infty$, then\n",
    "$$\\overline{l_{S}} \\to H(S)$$ \\qed\n",
    "\n",
    "### Shannon's first theorem\n",
    "\n",
    "* Analogy: how to buy things online without paying for delivery :)\n",
    "    * FanCourier taxes 15 lei per delivery\n",
    "        * not efficient to buy something worth a few lei\n",
    "    * How to improve efficiency? Buy $n$ things bundled together!\n",
    "    * The delivery cost **per unit** is now $\\frac{15}{n}$\n",
    "    * As $n \\to \\infty$, the delivery cost per unit $\\to 0$\n",
    "        * What's 15 lei when you pay $\\infty$ lei...\n",
    "\n",
    "### Shannon's first theorem\n",
    "\n",
    "Comments:\n",
    "\n",
    " * Shannon's first theorem shows that we can approach H(S) \n",
    " to any desired accuracy using extensions of large order of the source\n",
    "     * This is not practical: the size of $S^n$ gets too large for large $n$\n",
    "     * Other (better) algorithms than Shannon coding are used in practice to approach $H(S)$\n",
    "\n",
    "\n",
    "\n",
    "### Coding with the wrong code\n",
    "\n",
    "* Consider a source with probabilities $p(s_i)$\n",
    "* We use a code designed for a different source: $l_i = -\\log(q(s_i))$\n",
    "* The message probabilities are $p(s_i)$ but the code is designed for $q(s_i)$\n",
    "\\ \n",
    "\n",
    "* Examples:\n",
    "    * design a code based on a sample data file (like in lab)\n",
    "    * but we use it to encode various other files => probabilities might differ slightly\n",
    "    * e.g. design a code based a Romanian text, but encode a text in English\n",
    "\\ \n",
    "\n",
    "* What happens?\n",
    "\n",
    "### Coding with the wrong code\n",
    "\n",
    "* We lose some efficiency:\n",
    "    * Codeword lengths $\\overline{l_i}$ are not optimal for our source => increased $\\overline{l}$\n",
    "\n",
    "* If code were optimal, best average length = entropy $H(S)$:\n",
    "$$\\overline{l_{optimal}} = -\\sum{p(s_i) \\log{p(s_i)}}$$\n",
    "\n",
    "* But the actual average length we obtain is: \n",
    "$$\\overline{l_{actual}} = \\sum{p(s_i) l_i} = -\\sum{p(s_i) \\log{q(s_i)}}$$\n",
    "\n",
    "### The Kullback–Leibler distance\n",
    "\n",
    "* Difference between average lengths is:\n",
    "\n",
    "$$\\overline{l_{actual}} - \\overline{l_{optimal}} = \\sum_i{p(s_i) \\log(\\frac{p(s_i)}{q(s_i)})} = D_{KL}(p || q)$$\n",
    "\n",
    "* The difference  = **the Kullback-Leibler distance** between the two distributions\n",
    "    * is always $\\geq 0$ => improper code means increased $\\overline{l}$ (bad)\n",
    "    * distributions more different => larger average length (worse)\n",
    "    \n",
    "* The KL distance between the distributions = the number of extra bits used because \n",
    "of a code optimized for a different distribution $q(s_i)$ than the true distribution\n",
    "of our data $p(s_i)$\n",
    "\n",
    "### The Kullback–Leibler distance\n",
    "\n",
    "Reminder: where is the Kullback–Leibler distance used\n",
    "\n",
    "* Here: Using a code optimized for a different distribution:\n",
    "    * Average length is increased with $D_{KL}(p || q)$\n",
    "\n",
    "* In chapter IV (Channels): Definition of mutual information:\n",
    "    * Distance between $p(x_i \\cap y_j)$ and the distribution of two independent variables $p(x_i) \\cdot p(y_j)$\n",
    "$$I(X,Y) = \\sum_{i,j} p(x_i \\cap y_j) \\log(\\frac{p(x_i \\cap y_j)}{p(x_i)p(y_j)})$$\n",
    "\n",
    "\n",
    "### Shannon-Fano coding (binary)\n",
    "\n",
    "Shannon-Fano (binary) coding procedure:\n",
    "\n",
    "1. Sort the message probabilities in descending order\n",
    "2. Split into two subgroups as nearly equal as possible\n",
    "3. Assign first bit $0$ to first group, first bit $1$ to second group\n",
    "4. Repeat on each subgroup\n",
    "5. When reaching one single message => that is the codeword\n",
    "\n",
    "Example: blackboard\n",
    "\n",
    "Comments:\n",
    "\n",
    "* Shannon-Fano coding does not always produce the shortest code lengths\n",
    "* Connection: yes-no answers (example from first chapter)\n",
    "\n",
    "### Huffman coding (binary)\n",
    "\n",
    "Huffman coding procedure (binary):\n",
    "\n",
    "1. Sort the message probabilities in descending order\n",
    "2. Join the last two probabilities, insert result into existing list, preserve descending order\n",
    "3. Repeat until only two messages are remaining\n",
    "4. Assign first bit $0$ and $1$ to the final two messages\n",
    "5. Go back step by step: every time we had a sum, append $0$ and $1$ to the end of existing codeword\n",
    "\n",
    "Example: blackboard\n",
    "\n",
    "### Properties of Huffman coding\n",
    "Properties of Huffman coding:\n",
    "\n",
    "* Produces a code with the **smallest average length** (better than Shannon-Fano)\n",
    "* Assigning $0$ and $1$ can be done in any order => different codes, same lengths\n",
    "* When inserting a sum into an existing list, may be equal to another value => options\n",
    "    * we can insert above, below or in-between equal values\n",
    "    * leads to codes with different *individual* lengths, but same *average* length\n",
    "* Some better algorithms exist which do not assign a codeword to every single message\n",
    "(they code a while sequence at once, not every message)\n",
    "\n",
    "### Huffman coding (M symbols)\n",
    "\n",
    "General Huffman coding procedure for codes with $M$ symbols:\n",
    "\n",
    "* Have $M$ symbols $\\left\\lbrace x_1, x_2, ... x_M \\right\\rbrace$\n",
    "* Add together the last $M$ symbols\n",
    "* When assigning symbols, assign all $M$ symbols\n",
    "* **Important**: at the final step must have $M$ remaining values\n",
    "    * May be necessary to add *virtual* messages with probability 0 at the end of the initial list,\n",
    "    to end up with exactly $M$ messages in the last step\n",
    "    \n",
    "* Example : blackboard\n",
    "\n",
    "### Example: compare Huffman and Shannon-Fano\n",
    "\n",
    "Example: compare binary Huffman and Shannon-Fano for:\n",
    "$$p(s_i) = \\left\\lbrace 0.35, 0.17, 0.17, 0.16, 0.15 \\right\\rbrace$$\n",
    "\n",
    "### Probability of symbols\n",
    "\n",
    "* For every symbol $x_i$\n",
    "we can compute the average number of symbols $x_i$ in a code\n",
    "$$\\overline{l_{x_i}} = \\sum_i p(s_i) l_{x_i}(s_i)$$\n",
    "    * $l_{x_i}(s_i) =$ number of symbols $x_i$ in the codeword of $s_i$\n",
    "    * e.g.: average number of 0's and 1's in a code\n",
    "* Divide by average length => probability (frequency) of symbol $x_i$\n",
    "$$p(x_i) = \\frac{\\overline{l_{x_i}}}{\\overline{l}}$$\n",
    "\n",
    "* These are the probabilities of the input symbols for the transmission channel\n",
    "    * they play an important role in Chapter IV (transmission channels)\n",
    "\n",
    "### Source coding as data compression\n",
    "\n",
    "* Consider that the messages are already written in a binary code\n",
    "    * Example: characters in ASCII code\n",
    "\n",
    "* Source coding  = remapping the original codewords to other codewords \n",
    "    * The new codewords are shorter, on average\n",
    "    \n",
    "* This means data **compression**\n",
    "    * Just like the example in lab session\n",
    "    \n",
    "* What does data compression remove?\n",
    "    * Removes **redundancy**: unused bits, patterns, regularities etc.\n",
    "    * If you can guess somehow the next bit in a sequence, it means the bit is not really necessary,\n",
    "    so compression will remove it\n",
    "    * The compressed sequence looks like random data: impossible to guess, \n",
    "    no discernable patterns\n",
    "\n",
    "### Discussion: data compression with coding\n",
    "\n",
    "* Consider data compression with Shannon or Huffman coding, like we did in lab\n",
    "    * What property do we *exploit* in order to obtain compression?\n",
    "    * How does *compressible data* look like?\n",
    "    * How does *incompressible data* look like?\n",
    "    * What are the limitation of our data compression method?\n",
    "    * How could it be improved?\n",
    "\n",
    "### Other codes: arithmetic coding\n",
    "\n",
    "* Other types of coding do exist (info only)\n",
    "\t* Arithmetic coding\n",
    "\t* Adaptive schemes\n",
    "\t* etc.\n",
    "\n",
    "### Chapter summary\n",
    "\n",
    "* Average length: $\\overline{l} = \\sum_i p(s_i) l_i$\n",
    "* Code types: instantaneous $\\subset$ uniquely decodable $\\subset$ non-singular\n",
    "* All instantaneous or uniqualy decodable code must obey Kraft:\n",
    "$$ \\sum_i D^{-l_i} \\leq 1$$\n",
    "* Optimal codes: $l_i = -\\log(p(s_i))$, $\\overline{l_{min}} = H(S)$\n",
    "* Shannon's first theorem: use $n$-th order extension of $S$, $S^n$:\n",
    "$$\\boxed{H(S) \\leq \\overline{l_{S}} < H(S) + \\frac{1}{n}}$$\n",
    "    * average length always larger, but as close as desired to $H(S)$\n",
    "* Coding techniques:\n",
    "    * Shannon: ceil the optimal codeword lengths (round to upper)\n",
    "    * Shannon-Fano: split in two groups approx. equal\n",
    "    * Huffman: group last two. Is best of all."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md,.myst.md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
